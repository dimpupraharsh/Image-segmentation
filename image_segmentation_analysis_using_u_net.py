# -*- coding: utf-8 -*-
"""Image segmentation analysis using U-NET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xzqv6eM5cyttM8x77GMaW0uRGq-OBA9O

## Importing Necessary Libraries ##
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import collections
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pycocotools.coco import COCO
import skimage.io as io
from tabulate import tabulate
from itertools import combinations
import uuid
import random
import numpy as np
from pycocotools.coco import COCO
import skimage.io as io
from tensorflow.keras.utils import Sequence
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from PIL import Image
import tensorflow as tf

# --- OUTPUT DIRECTORY ---
# Define directory to save plots
output_dir = "./eda_plots"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# --- SET PLOT STYLE ---
# Configure Seaborn for a clean, professional look
sns.set(style="whitegrid", font_scale=1.2, rc={
    "axes.facecolor": "#E6F3F0",  # Calm mint green background
    "figure.facecolor": "#E6F3F0",
    "grid.color": "#D1E8E2",
    "axes.edgecolor": "#333333",
    "axes.labelcolor": "#333333",
    "xtick.color": "#333333",
    "ytick.color": "#333333",
    "font.family": "Arial",
    "axes.labelweight": "bold"  # Bold axis labels
})

# --- PATH CONFIGURATION ---
# Define dataset paths for train, validation, and test sets
train_data_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300"
val_data_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300"
test_data_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30"

train_annotations = os.path.join(train_data_dir, "labels.json")
val_annotations = os.path.join(val_data_dir, "labels.json")

# --- INITIALIZE COCO API ---
# Load COCO annotations for train and validation datasets
coco_train_data = COCO(train_annotations)
coco_val_data = COCO(val_annotations)

# --- CLASS DEFINITION ---
# Define target classes for analysis
analysis_classes = ["cake", "car", "dog", "person"]

# --- UTILITY FUNCTIONS ---

def fetch_category_name(class_id, categories):
    """Retrieve category name for a given class ID."""
    for category in categories:
        if category["id"] == class_id:
            return category["name"]
    return "None"

def count_dataset_images(image_dir):
    """Count the number of image files (.jpg or .png) in a directory."""
    return len([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))])

def extract_image_dimensions(coco, image_ids, image_dir=None):
    """Extract width and height of images from COCO dataset ."""
    widths, heights = [], []
    if coco and image_ids:
        for img_id in image_ids:
            img_info = coco.loadImgs(img_id)[0]
            widths.append(img_info['width'])
            heights.append(img_info['height'])
    elif image_dir:
        for img_file in os.listdir(image_dir):
            if img_file.endswith(('.jpg', '.png')):
                img_path = os.path.join(image_dir, img_file)
                img = io.imread(img_path)
                heights.append(img.shape[0])
                widths.append(img.shape[1])
    return np.array(widths), np.array(heights)

# --- EDA ANALYSIS FUNCTIONS ---

def overview_analysis(coco, dataset_name, img_dir=None):
    """Generate summary statistics for a dataset split."""
    if coco:  # Train or Validation
        img_ids = coco.getImgIds()
        categories = coco.loadCats(coco.getCatIds())
        target_cat_ids = coco.getCatIds(catNms=analysis_classes)
        instance_count = len(coco.getAnnIds(catIds=target_cat_ids))
        return img_ids, target_cat_ids, categories, instance_count
    else:  # Test
        img_count = count_dataset_images(img_dir)
        return None, None, None, "N/A"

def image_distribution_analysis(train_ids, val_ids, test_img_count):
    """Analyze the number of images across dataset splits."""
    train_count = len(train_ids) if train_ids else 0
    val_count = len(val_ids) if val_ids else 0
    test_count = test_img_count if test_img_count else 0
    return train_count, val_count, test_count

def class_instance_analysis(coco_train, coco_val, train_cat_ids, val_cat_ids, train_cats, val_cats):
    """Analyze class instance distribution in train and validation sets."""
    train_dist = {cls: len(coco_train.getAnnIds(catIds=coco_train.getCatIds(catNms=[cls]))) for cls in analysis_classes}
    val_dist = {cls: len(coco_val.getAnnIds(catIds=coco_val.getCatIds(catNms=[cls]))) for cls in analysis_classes}
    return train_dist, val_dist

def dimension_stats_analysis(coco_train, coco_val, test_dir, train_ids, val_ids):
    """Analyze image dimensions across dataset splits with enhanced visualizations."""
    train_widths, train_heights = extract_image_dimensions(coco_train, train_ids)
    val_widths, val_heights = extract_image_dimensions(coco_val, val_ids)
    test_widths, test_heights = extract_image_dimensions(None, None, test_dir)

    table_data = [
        ["Train", f"{train_widths.mean():.1f}", train_widths.min(), train_widths.max(), f"{train_heights.mean():.1f}", train_heights.min(), train_heights.max()],
        ["Validation", f"{val_widths.mean():.1f}", val_widths.min(), val_widths.max(), f"{val_heights.mean():.1f}", val_heights.min(), val_heights.max()],
        ["Test", f"{test_widths.mean():.1f}", test_widths.min(), test_widths.max(), f"{test_heights.mean():.1f}", test_heights.min(), test_heights.max()]
    ]
    headers = ["Split", "Mean_Width", "Min_Width", "Max_Width", "Mean_Height", "Min_Height", "Max_Height"]
    print("\Image Dimension Statistics")
    print("==================================================")
    print("Image Size Stats:")
    print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

    plt.figure(figsize=(18, 12))
    plt.subplot(2, 3, 1)
    plt.hist(train_widths, bins=30, color='#4A919E', edgecolor='white', alpha=0.85)
    plt.title("Train Data: Width Distribution", fontsize=14, weight='bold')
    plt.xlabel("Width (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.subplot(2, 3, 2)
    plt.hist(val_widths, bins=30, color='#F4A261', edgecolor='white', alpha=0.85)
    plt.title("Validation Data: Width Distribution", fontsize=14, weight='bold')
    plt.xlabel("Width (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.subplot(2, 3, 3)
    plt.hist(test_widths, bins=30, color='#6B728E', edgecolor='white', alpha=0.85)
    plt.title("Test Data: Width Distribution", fontsize=14, weight='bold')
    plt.xlabel("Width (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.subplot(2, 3, 4)
    plt.hist(train_heights, bins=30, color='#4A919E', edgecolor='white', alpha=0.85)
    plt.title("Train Data: Height Distribution", fontsize=14, weight='bold')
    plt.xlabel("Height (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.subplot(2, 3, 5)
    plt.hist(val_heights, bins=30, color='#F4A261', edgecolor='white', alpha=0.85)
    plt.title("Validation Data: Height Distribution", fontsize=14, weight='bold')
    plt.xlabel("Height (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.subplot(2, 3, 6)
    plt.hist(test_heights, bins=30, color='#6B728E', edgecolor='white', alpha=0.85)
    plt.title("Test Data: Height Distribution", fontsize=14, weight='bold')
    plt.xlabel("Height (pixels)", fontsize=12, weight='bold', color='black')
    plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')

    plt.tight_layout(pad=2.0)
    plt.savefig(os.path.join(output_dir, "image_dimension_distribution.png"), dpi=300, bbox_inches='tight')
    plt.show()

def aspect_ratio_stats(coco_train, coco_val, test_dir, train_ids, val_ids):
    """Calculate aspect ratio distributions across dataset splits."""
    train_widths, train_heights = extract_image_dimensions(coco_train, train_ids)
    val_widths, val_heights = extract_image_dimensions(coco_val, val_ids)
    test_widths, test_heights = extract_image_dimensions(None, None, test_dir)
    return train_widths / train_heights, val_widths / val_heights, test_widths / test_heights

def object_count_distribution(coco_train, coco_val, test_coco, train_ids, val_ids):
    """Analyze the number of objects per image in train and validation sets."""
    train_counts = [len(coco_train.getAnnIds(imgIds=img_id)) for img_id in train_ids] if coco_train else []
    val_counts = [len(coco_val.getAnnIds(imgIds=img_id)) for img_id in val_ids] if coco_val else []
    test_counts = [] if not test_coco else []
    return train_counts, val_counts, test_counts

def segmentation_format_counts(coco_train, coco_val, train_cat_ids, val_cat_ids):
    """Count segmentation types (Polygon vs RLE) in train and validation sets."""
    if not coco_train or not coco_val:
        return None, None, None, None

    train_ann_ids = coco_train.getAnnIds(catIds=train_cat_ids)
    train_anns = coco_train.loadAnns(train_ann_ids)
    train_poly_count = sum(1 for ann in train_anns if isinstance(ann['segmentation'], list))
    train_rle_count = sum(1 for ann in train_anns if not isinstance(ann['segmentation'], list))

    val_ann_ids = coco_val.getAnnIds(catIds=val_cat_ids)
    val_anns = coco_val.loadAnns(val_ann_ids)
    val_poly_count = sum(1 for ann in val_anns if isinstance(ann['segmentation'], list))
    val_rle_count = sum(1 for ann in val_anns if not isinstance(ann['segmentation'], list))
    return train_poly_count, train_rle_count, val_poly_count, val_rle_count

def class_cooccurrence_matrix(coco, target_cat_ids, categories, dataset_name):
    """Generate class co-occurrence matrix for a dataset split."""
    if not coco:
        return None
    class_names = [fetch_category_name(cat_id, categories) for cat_id in target_cat_ids]
    n_classes = len(class_names)
    cooc_matrix = np.zeros((n_classes, n_classes), dtype=int)

    img_ids = coco.getImgIds()
    for img_id in img_ids:
        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=target_cat_ids)
        anns = coco.loadAnns(ann_ids)
        img_cats = set(ann['category_id'] for ann in anns)
        for cat1, cat2 in combinations(img_cats, 2):
            idx1 = target_cat_ids.index(cat1)
            idx2 = target_cat_ids.index(cat2)
            cooc_matrix[idx1, idx2] += 1
            cooc_matrix[idx2, idx1] += 1
    return cooc_matrix, class_names

# --- EXECUTE EDA ---

# 1. Dataset Overview
print("\n Dataset Overview")
print("==================================================")
train_img_ids, train_cat_ids, train_cats, train_instances = overview_analysis(coco_train_data, "Train")
val_img_ids, val_cat_ids, val_cats, val_instances = overview_analysis(coco_val_data, "Validation")
test_img_ids, test_cat_ids, test_cats, test_instances = overview_analysis(None, "Test", test_data_dir)
test_img_count = count_dataset_images(test_data_dir)

table_data = [
    ["Train", len(train_img_ids), train_instances],
    ["Validation", len(val_img_ids), val_instances],
    ["Test", test_img_count, test_instances]
]
headers = ["Split", "Images", "Total_Instances"]
print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

# 2. Image and Class Distribution
print("\n Image and Class Distribution Analysis")
print("==================================================")
train_count, val_count, test_count = image_distribution_analysis(train_img_ids, val_img_ids, test_img_count)
train_dist, val_dist = class_instance_analysis(coco_train_data, coco_val_data, train_cat_ids, val_cat_ids, train_cats, val_cats)
total_dist = {cls: train_dist[cls] + val_dist[cls] for cls in analysis_classes}

plt.figure(figsize=(18, 6))
plt.subplot(1, 3, 1)
bars = plt.bar(['Train', 'Validation', 'Test'], [train_count, val_count, test_count], color=['#4A919E', '#F4A261', '#6B728E'], edgecolor='white', alpha=0.85)
plt.title("Image Count Across Splits", fontsize=14, weight='bold')
plt.ylabel("Number of Images", fontsize=12, weight='bold', color='black')
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}', ha='center', va='bottom', fontsize=10, color='#333333')

plt.subplot(1, 3, 2)
x = np.arange(len(analysis_classes))
bars1 = plt.bar(x - 0.2, [train_dist[cls] for cls in analysis_classes], 0.4, label='Train', color='#A3D8C5', edgecolor='white', alpha=0.85)
bars2 = plt.bar(x + 0.2, [val_dist[cls] for cls in analysis_classes], 0.4, label='Validation', color='#F4C4A1', edgecolor='white', alpha=0.85)
plt.xlabel("Classes", fontsize=12, weight='bold', color='black')
plt.ylabel("Number of Instances", fontsize=12, weight='bold', color='black')
plt.title("Target Class Instance Count ", fontsize=14, weight='bold')
plt.xticks(x, analysis_classes, fontsize=10)
plt.legend(fontsize=10)
for bar in bars1:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}', ha='center', va='bottom', fontsize=10, color='#333333')
for bar in bars2:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}', ha='center', va='bottom', fontsize=10, color='#333333')

plt.subplot(1, 3, 3)
plt.pie(total_dist.values(), labels=total_dist.keys(), autopct='%1.1f%%', colors=['#A3D8C5', '#F4C4A1', '#D1C4E9', '#FFE4B5'], textprops={'fontsize': 10, 'color': '#333333'})
plt.title("Target Class Distribution", fontsize=14, weight='bold')
plt.tight_layout(pad=2.0)
plt.savefig(os.path.join(output_dir, "image_class_distribution.png"), dpi=300, bbox_inches='tight')
plt.show()

table_data = [
    [cls, train_dist[cls], val_dist[cls], "N/A", train_dist[cls] + val_dist[cls]]
    for cls in analysis_classes
]
headers = ["Class", "Train_Count", "Val_Count", "Test_Count", "Total"]
print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

# 3. Image Dimension Analysis
dimension_stats_analysis(coco_train_data, coco_val_data, test_data_dir, train_img_ids, val_img_ids)

# 4. Aspect Ratio and Object Count Distribution
print("\n Aspect Ratio and Object Count Analysis")
print("==================================================")
train_ratios, val_ratios, test_ratios = aspect_ratio_stats(coco_train_data, coco_val_data, test_data_dir, train_img_ids, val_img_ids)
train_obj_counts, val_obj_counts, test_obj_counts = object_count_distribution(coco_train_data, coco_val_data, None, train_img_ids, val_img_ids)

plt.figure(figsize=(18, 6))
plt.subplot(1, 3, 1)
plt.hist(train_ratios, bins=30, alpha=0.6, label='Train', color='#A3D8C5', edgecolor='white')
plt.hist(val_ratios, bins=30, alpha=0.6, label='Validation', color='#F4C4A1', edgecolor='white')
plt.hist(test_ratios, bins=30, alpha=0.6, label='Test', color='#6B728E', edgecolor='white')
plt.title("Aspect Ratio Distribution Across Splits", fontsize=14, weight='bold')
plt.xlabel("Aspect Ratio (Width/Height)", fontsize=12, weight='bold', color='black')
plt.ylabel("Frequency", fontsize=12, weight='bold', color='black')
plt.legend(fontsize=10)

plt.subplot(1, 3, 2)
plt.hist(train_obj_counts, bins=20, color='#F4A261', edgecolor='white', alpha=0.85)
plt.title("Train Objects per Image", fontsize=14, weight='bold')
plt.xlabel("Number of Objects", fontsize=12, weight='bold', color='black')
plt.ylabel("Number of Images", fontsize=12, weight='bold', color='black')

plt.subplot(1, 3, 3)
plt.hist(val_obj_counts, bins=20, color='#4A919E', edgecolor='white', alpha=0.85)
plt.title("Validation Objects per Image", fontsize=14, weight='bold')
plt.xlabel("Number of Objects", fontsize=12, weight='bold', color='black')
plt.ylabel("Number of Images", fontsize=12, weight='bold', color='black')
plt.tight_layout(pad=2.0)
plt.savefig(os.path.join(output_dir, "aspect_ratio_object_count.png"), dpi=300, bbox_inches='tight')
plt.show()

print("\nTest Set Objects per Image: N/A (No annotations)")

# 5. Segmentation and Co-occurrence Analysis
print("\n Segmentation and Class Co-occurrence Analysis")
print("==================================================")
train_poly, train_rle, val_poly, val_rle = segmentation_format_counts(coco_train_data, coco_val_data, train_cat_ids, val_cat_ids)
train_cooc, train_names = class_cooccurrence_matrix(coco_train_data, train_cat_ids, train_cats, "Train")
val_cooc, val_names = class_cooccurrence_matrix(coco_val_data, val_cat_ids, val_cats, "Validation")

plt.figure(figsize=(18, 6))
plt.subplot(1, 3, 1)
plt.bar(['Train Polygon', 'Train RLE', 'Val Polygon', 'Val RLE'], [train_poly, train_rle, val_poly, val_rle], color=['#A3D8C5', '#4A919E', '#F4C4A1', '#6B728E'], edgecolor='white', alpha=0.85)
plt.title("Segmentation Type Counts", fontsize=14, weight='bold')
plt.ylabel("Number of Instances", fontsize=12, weight='bold', color='black')
for i, v in enumerate([train_poly, train_rle, val_poly, val_rle]):
    plt.text(i, v, str(v), ha='center', va='bottom', fontsize=10, color='#333333')
plt.xticks(rotation=45, fontsize=10)

plt.subplot(1, 3, 2)
if train_cooc is not None:
    sns.heatmap(train_cooc, annot=True, fmt='d', cmap='BuGn', xticklabels=train_names, yticklabels=train_names, cbar_kws={'label': 'Co-occurrence Count'})
    plt.title("Train Class Co-occurrence Heatmap", fontsize=14, weight='bold')
    plt.xlabel("Class", fontsize=12, weight='bold', color='black')
    plt.ylabel("Class", fontsize=12, weight='bold', color='black')

plt.subplot(1, 3, 3)
if val_cooc is not None:
    sns.heatmap(val_cooc, annot=True, fmt='d', cmap='BuGn', xticklabels=val_names, yticklabels=val_names, cbar_kws={'label': 'Co-occurrence Count'})
    plt.title("Validation Class Co-occurrence Heatmap", fontsize=14, weight='bold')
    plt.xlabel("Class", fontsize=12, weight='bold', color='black')
    plt.ylabel("Class", fontsize=12, weight='bold', color='black')
plt.tight_layout(pad=2.0)
plt.savefig(os.path.join(output_dir, "segmentation_cooccurrence.png"), dpi=300, bbox_inches='tight')
plt.show()

print("\nSegmentation Type Counts:")
print(f"  Train Polygon masks: {train_poly}")
print(f"  Train RLE masks: {train_rle}")
print(f"  Validation Polygon masks: {val_poly}")
print(f"  Validation RLE masks: {val_rle}")
print("\nTest Set Segmentation Type Counts: N/A (No annotations)")

print("\nClass Co-occurrence Analysis:")
if train_cooc is None:
    print("Train Class Co-occurrence Heatmap: N/A (No annotations)")
else:
    print("Train Class Co-occurrence Heatmap generated")
if val_cooc is None:
    print("Validation Class Co-occurrence Heatmap: N/A (No annotations)")
else:
    print("Validation Class Co-occurrence Heatmap generated")

"""## Data preparation for U-NET Model"""

# Define color map for visualization
color_map = {
    0: [0, 0, 0],        # background - black
    1: [255, 0, 0],      # cake - red
    2: [0, 255, 0],      # car - green
    3: [0, 0, 255],      # dog - blue
    4: [255, 255, 0],    # person - yellow
}

def mask_to_rgb(mask):
    """Convert 2D mask (H,W) with integer labels to RGB image."""
    h, w = mask.shape
    rgb_mask = np.zeros((h, w, 3), dtype=np.uint8)
    for class_idx, color in color_map.items():
        rgb_mask[mask == class_idx] = color
    return rgb_mask

# Evaluation metrics
def compute_metrics(y_true, y_pred, num_classes=5):
    """Compute pixel-wise accuracy, mean IoU, per-class IoU, and mean Dice."""
    y_true = y_true.astype(np.int32).flatten()
    y_pred = y_pred.astype(np.int32).flatten()

    # Pixel-wise accuracy
    accuracy = np.mean(y_true == y_pred)

    # IoU and Dice per class
    iou_per_class = []
    dice_per_class = []
    for c in range(num_classes):
        true_c = (y_true == c)
        pred_c = (y_pred == c)
        intersection = np.sum(true_c & pred_c)
        union = np.sum(true_c | pred_c)
        true_sum = np.sum(true_c)
        pred_sum = np.sum(pred_c)

        # IoU
        iou = intersection / union if union > 0 else 1.0 if intersection == 0 else 0.0
        iou_per_class.append(iou)

        # Dice
        dice = (2 * intersection) / (true_sum + pred_sum) if (true_sum + pred_sum) > 0 else 1.0 if intersection == 0 else 0.0
        dice_per_class.append(dice)

    return {
        'pixel_accuracy': accuracy,
        'mean_iou': np.mean([iou for iou in iou_per_class if not np.isnan(iou)]),
        'mean_dice': np.mean([dice for dice in dice_per_class if not np.isnan(dice)]),
        'iou_per_class': iou_per_class,
        'dice_per_class': dice_per_class
    }

def print_metrics(metrics, class_names=['background', 'cake', 'car', 'dog', 'person']):
    """Print evaluation metrics."""
    print(f"Pixel-wise Accuracy: {metrics['pixel_accuracy']:.4f}")
    print(f"Mean IoU: {metrics['mean_iou']:.4f}")
    print(f"Mean Dice: {metrics['mean_dice']:.4f}")
    print("Per-class IoU:")
    for c, iou in enumerate(metrics['iou_per_class']):
        print(f"  {class_names[c]}: {iou:.4f}")
    print("Per-class Dice:")
    for c, dice in enumerate(metrics['dice_per_class']):
        print(f"  {class_names[c]}: {dice:.4f}")

class CocoSegmentationGenerator(Sequence):
    def __init__(self, image_dir, ann_file, target_classes, image_size=(224, 224), batch_size=32, augment=False):
        super().__init__()
        self.image_dir = image_dir
        self.coco = COCO(ann_file)
        self.target_class_ids = self.coco.getCatIds(catNms=target_classes)
        print(f"Target class IDs: {self.target_class_ids}")

        # Get images per class and oversample
        image_ids_per_class = {cat_id: self.coco.getImgIds(catIds=[cat_id]) for cat_id in self.target_class_ids}
        max_images = max(len(ids) for ids in image_ids_per_class.values())
        self.image_ids = []
        self.class_labels = []  # Track class labels for each image
        for cat_id, img_ids in image_ids_per_class.items():
            oversample_factor = max(1, max_images // max(1, len(img_ids)))
            self.image_ids.extend(img_ids * oversample_factor)
            self.class_labels.extend([cat_id] * len(img_ids) * oversample_factor)
        random.shuffle(self.image_ids)
        print(f"Number of images after oversampling: {len(self.image_ids)}")

        self.class_id_to_index = {cat_id: idx+1 for idx, cat_id in enumerate(self.target_class_ids)}
        self.image_size = image_size
        self.batch_size = batch_size
        self.augment = augment

        # Standard augmentation
        self.data_aug = ImageDataGenerator(
            rotation_range=10,
            zoom_range=0.1,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True,
            vertical_flip=True,
            fill_mode='nearest'
        )

        # Stronger augmentation for minority classes (cake, dog)
        self.strong_data_aug = ImageDataGenerator(
            rotation_range=20,
            zoom_range=0.2,
            width_shift_range=0.3,
            height_shift_range=0.3,
            horizontal_flip=True,
            vertical_flip=True,
            shear_range=0.2,
            fill_mode='nearest'
        )

    def __len__(self):
        return max(1, (len(self.image_ids) + self.batch_size - 1) // self.batch_size)

    def __getitem__(self, idx):
        start_idx = idx * self.batch_size
        end_idx = min((idx + 1) * self.batch_size, len(self.image_ids))
        batch_img_ids = self.image_ids[start_idx:end_idx]
        batch_labels = self.class_labels[start_idx:end_idx]
        images, masks = [], []

        for img_id, label in zip(batch_img_ids, batch_labels):
            img_info = self.coco.loadImgs(img_id)[0]
            img_path = os.path.join(self.image_dir, img_info['file_name'])
            if not os.path.exists(img_path):
                print(f"Warning: Image not found at {img_path}")
                continue

            image = load_img(img_path)
            image = image.resize(self.image_size, Image.Resampling.LANCZOS)
            image = img_to_array(image) / 255.0
            image = tf.keras.applications.resnet50.preprocess_input(image * 255.0)

            mask = np.zeros(self.image_size, dtype=np.uint8)
            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.target_class_ids)
            anns = self.coco.loadAnns(ann_ids)

            # Prioritize lower class indices
            for ann in sorted(anns, key=lambda x: self.class_id_to_index.get(x["category_id"], 0)):
                class_id = ann["category_id"]
                class_idx = self.class_id_to_index.get(class_id, 0)
                if class_idx == 0:
                    continue
                ann_mask = self.coco.annToMask(ann)
                ann_mask_resized = Image.fromarray(ann_mask).resize(self.image_size, Image.Resampling.NEAREST)
                ann_mask_resized = (np.array(ann_mask_resized) > 0).astype(np.uint8) * class_idx
                mask[ann_mask_resized > 0] = class_idx

            mask = np.expand_dims(mask, axis=-1)

            if self.augment:
                seed = random.randint(0, 99999)
                # Apply stronger augmentation for minority classes
                aug = self.strong_data_aug if class_id in self.target_class_ids[:3] else self.data_aug
                image = aug.random_transform(image, seed=seed)
                mask = aug.random_transform(mask, seed=seed)
                mask = np.round(mask).astype(np.uint8)

            images.append(image)
            masks.append(mask)

        if not images:
            print(f"Warning: No valid images in batch {idx}. Returning empty batch.")
            return np.array([], dtype=np.float32), np.array([], dtype=np.uint8)

        return np.array(images, dtype=np.float32), np.array(masks, dtype=np.uint8)

def weighted_sparse_categorical_crossentropy(y_true, y_pred):
    """Custom loss with class weights."""
    weights = tf.constant([1.0, 20.0, 15.0, 12.0, 5.0], dtype=tf.float32)
    y_true = tf.cast(y_true, tf.int32)
    y_true_squeezed = tf.squeeze(y_true, axis=-1)
    weight_mask = tf.gather(weights, y_true_squeezed)
    losses = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
    weighted_losses = losses * weight_mask
    return tf.reduce_mean(weighted_losses)

def unet_resnet50_model(input_shape=(224, 224, 3), num_classes=5):
    inputs = tf.keras.Input(shape=input_shape)
    base_model = tf.keras.applications.ResNet50(
        weights='imagenet',
        include_top=False,
        input_tensor=inputs
    )
    base_model.trainable = False

    skip_connections = [
        base_model.get_layer('conv1_relu').output,        # 112x112
        base_model.get_layer('conv2_block3_out').output,  # 56x56
        base_model.get_layer('conv3_block4_out').output,  # 28x28
        base_model.get_layer('conv4_block6_out').output,  # 14x14
    ]
    encoder_output = base_model.get_layer('conv5_block3_out').output  # 7x7

    # Decoder
    x = tf.keras.layers.Conv2DTranspose(1024, (3, 3), strides=(2, 2), padding='same', activation='relu')(encoder_output)
    x = tf.keras.layers.Concatenate()([x, skip_connections[3]])
    x = tf.keras.layers.Conv2D(1024, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(1024, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Dropout(0.3)(x)

    x = tf.keras.layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = tf.keras.layers.Concatenate()([x, skip_connections[2]])
    x = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Dropout(0.3)(x)

    x = tf.keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = tf.keras.layers.Concatenate()([x, skip_connections[1]])
    x = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    x = tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = tf.keras.layers.Concatenate()([x, skip_connections[0]])
    x = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    x = tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)

    outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model,base_model

"""## Loading data_path"""

# Data paths
train_data_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300"
train_annotation_file = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json"
val_data_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300"
val_annotation_file = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json"

# Initialize generators
train_generator = CocoSegmentationGenerator(
    image_dir=train_data_path,
    ann_file=train_annotation_file,
    target_classes=["cake", "car", "dog", "person"],
    image_size=(224, 224),
    batch_size=32,
    augment=True
)

val_generator = CocoSegmentationGenerator(
    image_dir=val_data_path,
    ann_file=val_annotation_file,
    target_classes=["cake", "car", "dog", "person"],
    image_size=(224, 224),
    batch_size=32,
    augment=False
)

"""## U-Net Model Summary"""

# Create and compile model
model = unet_resnet50_model(input_shape=(224, 224, 3), num_classes=5)
model.compile(
    optimizer='adam',
    loss=weighted_sparse_categorical_crossentropy,
    metrics=['accuracy']
)
model.summary()

"""## Segmentation Mask for Training Images for Target class [cake,dog,car,person]"""

print("Inspecting annotations for all target classes in images containing minority classes...")
coco = COCO(train_annotation_file)
target_class_names = ["cake", "car", "dog", "person"]
target_class_ids = coco.getCatIds(catNms=target_class_names)

for cat_name in ["cake", "car", "dog","person"]:
    cat_id = coco.getCatIds(catNms=[cat_name])[0]
    img_ids = coco.getImgIds(catIds=[cat_id])[:2]

    for img_id in img_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(train_data_path, img_info['file_name'])
        image = load_img(img_path)
        image = image.resize((224, 224), Image.Resampling.LANCZOS)
        image = np.array(image) / 255.0

        mask = np.zeros((224, 224), dtype=np.uint8)

        # Get all annotations in this image
        ann_ids = coco.getAnnIds(imgIds=[img_id])
        anns = coco.loadAnns(ann_ids)

        # Draw masks for only the target classes (avoid drawing background or irrelevant objects)
        for ann in anns:
            ann_cat_id = ann["category_id"]
            if ann_cat_id not in target_class_ids:
                continue
            class_idx = train_generator.class_id_to_index[ann_cat_id]

            ann_mask = coco.annToMask(ann)
            ann_mask_resized = Image.fromarray(ann_mask).resize((224, 224), Image.Resampling.NEAREST)
            ann_mask_resized = (np.array(ann_mask_resized) > 0).astype(np.uint8)

            # Add class index only where not already labeled (preserve earlier class masks)
            mask = np.where((ann_mask_resized == 1) & (mask == 0), class_idx, mask)

        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.imshow(image)
        plt.title(f"Image ID: {img_id} ({cat_name})")
        plt.axis("off")
        plt.subplot(1, 2, 2)
        plt.imshow(mask_to_rgb(mask))
        plt.title(f"Combined Mask (Labels: {np.unique(mask)})")
        plt.axis("off")
        plt.show()

"""## Model Training"""

# Initial training
print("Initial training with frozen ResNet50...")
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=100,
    verbose=1
)

"""## Training Vs Validation [loss,Accuracy Plots] before fine-tuning"""

# Extract training and validation metrics from the history object
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']

# Find the epoch with the lowest validation loss and highest validation accuracy
index_loss = np.argmin(val_loss)
index_acc = np.argmax(val_acc)
val_lowest = val_loss[index_loss]
val_highest = val_acc[index_acc]

# Generate epochs
epochs = [i+1 for i in range(len(train_acc))]

# Define labels for best epochs
loss_label = f'Best Epoch = {str(index_loss + 1)}'
acc_label = f'Best Epoch = {str(index_acc + 1)}'

# Plot training and validation loss
plt.figure(figsize=(20, 8))
plt.style.use('fivethirtyeight')

plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'r--', label='Training Loss')
plt.plot(epochs, val_loss, 'g', label='Validation Loss')
plt.scatter(index_loss + 1, val_lowest, s=150, c='orange', label=loss_label)
plt.title('Training Vs Validation (Loss) before fine-tuning')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'r', label='Training Accuracy')
plt.plot(epochs, val_acc, 'g--', label='Validation Accuracy')
plt.scatter(index_acc + 1, val_highest, s=150, c='orange', label=acc_label)
plt.title('Training Vs Validation (Accuracy) before fine-tuning')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## Model Training & Plots after Fine-tuning"""

# Fine-tuning
model, base_model = unet_resnet50_model(input_shape=(224, 224, 3), num_classes=5)

print("Unfreezing last 40 layers of ResNet50 for fine-tuning...")
for layer in base_model.layers[-40:]:
    layer.trainable = True

# Recompile with lower learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=weighted_sparse_categorical_crossentropy,
    metrics=['accuracy']
)

# Fine-tune
print("Starting fine-tuning...")
history_finetune = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=30,
    verbose=1
)

# Extract training and validation metrics from the history object
train_acc = history_finetune.history['accuracy']
train_loss = history_finetune.history['loss']
val_acc = history_finetune.history['val_accuracy']
val_loss = history_finetune.history['val_loss']

# Find the epoch with the lowest validation loss and highest validation accuracy
index_loss = np.argmin(val_loss)
index_acc = np.argmax(val_acc)
val_lowest = val_loss[index_loss]
val_highest = val_acc[index_acc]

# Generate epochs
epochs = [i+1 for i in range(len(train_acc))]

# Define labels for best epochs
loss_label = f'Best Epoch = {str(index_loss + 1)}'
acc_label = f'Best Epoch = {str(index_acc + 1)}'

# Plot training and validation loss
plt.figure(figsize=(20, 8))
plt.style.use('fivethirtyeight')

plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'r--', label='Training Loss')
plt.plot(epochs, val_loss, 'g', label='Validation Loss')
plt.scatter(index_loss + 1, val_lowest, s=150, c='orange', label=loss_label)
plt.title('Training Vs Validation (Loss) after fine-tuning')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'r', label='Training Accuracy')
plt.plot(epochs, val_acc, 'g--', label='Validation Accuracy')
plt.scatter(index_acc + 1, val_highest, s=150, c='orange', label=acc_label)
plt.title('Training Vs Validation (Accuracy) after fine-tuning')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""### Saving the model

"""

# Save only the weights (recommended for your case)
model.save_weights('/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model/car_updated.weights.h5')
print("Model weights saved to /content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model/car_updated.weights.h5")

"""## Evaluating the Model On Validation Data"""

def denormalize_image(img):
    """Reverse ImageNet-style normalization for visualization."""
    mean = np.array([123.68, 116.779, 103.939])
    img = img + mean
    return np.clip(img / 255.0, 0, 1)

def evaluate_model1(model, generator, class_names=['background', 'cake', 'car', 'dog', 'person'],threshold=0.8):
    all_metrics = []
    class_counts = {c: 0 for c in range(5)}  # Track predicted class frequencies

    # Process only the first batch
    images, masks = generator[0]
    if images.size == 0:
        print("Skipping empty batch 0")
        return

    preds = model.predict(images, verbose=0)
    class_preds = np.argmax(preds, axis=-1)
    confidence = np.max(preds, axis=-1)
    preds = np.where(confidence >= threshold, class_preds, 0)

    for i in range(len(images)):
        metrics = compute_metrics(masks[i].squeeze(), preds[i], num_classes=5)
        all_metrics.append(metrics)

        # Update class counts
        unique, counts = np.unique(preds[i], return_counts=True)
        for cls, cnt in zip(unique, counts):
            class_counts[cls] += cnt

    # Visualize all images in batch
    plt.figure(figsize=(15, 5 * len(images)))
    for j in range(len(images)):
        plt.subplot(len(images), 3, 3*j + 1)
        plt.imshow(denormalize_image(images[j]))
        plt.title(f"Image {j}")
        plt.axis("off")

        plt.subplot(len(images), 3, 3*j + 2)
        plt.imshow(mask_to_rgb(masks[j].squeeze()))
        plt.title(f"Ground Truth (Labels: {np.unique(masks[j].squeeze())})")
        plt.axis("off")

        plt.subplot(len(images), 3, 3*j + 3)
        plt.imshow(mask_to_rgb(preds[j]))
        plt.title(f"Prediction (Labels: {np.unique(preds[j])})")
        plt.axis("off")

    plt.tight_layout()
    #plt.savefig("val_batch_0_visualization.png")
    plt.show()

    # Print predicted class distribution
    print("Predicted class distribution (pixel counts):")
    for cls, cnt in class_counts.items():
        print(f"  {class_names[cls]}: {cnt}")

    # Aggregate metrics
    final_metrics = {
        'pixel_accuracy': np.mean([m['pixel_accuracy'] for m in all_metrics]),
        'mean_iou': np.mean([m['mean_iou'] for m in all_metrics]),
        'mean_dice': np.mean([m['mean_dice'] for m in all_metrics]),
        'iou_per_class': np.mean([m['iou_per_class'] for m in all_metrics], axis=0),
        'dice_per_class': np.mean([m['dice_per_class'] for m in all_metrics], axis=0)
    }

    print("\nValidation Metrics (Single Batch):")
    print_metrics(final_metrics, class_names)
    return final_metrics

# Run evaluation
evaluate_model1(model, val_generator, threshold=0.8)

"""## Finally Testing the Model on Test Dataset"""

class TestImageGenerator(Sequence):
    def __init__(self, image_dir, image_size=(224, 224), batch_size=1):
        super().__init__()
        self.image_dir = image_dir
        self.image_size = image_size
        self.batch_size = batch_size
        # List image files
        self.image_ids = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]
        if len(self.image_ids) < 30:
            print(f"Warning: Found only {len(self.image_ids)} images in {image_dir}")
        print(f"Number of test images: {len(self.image_ids)}")

    def __len__(self):
        return max(1, (len(self.image_ids) + self.batch_size - 1) // self.batch_size)

    def __getitem__(self, idx):
        start_idx = idx * self.batch_size
        end_idx = min((idx + 1) * self.batch_size, len(self.image_ids))
        batch_img_ids = self.image_ids[start_idx:end_idx]
        images, raw_images, filenames = [], [], []

        for img_id in batch_img_ids:
            img_path = os.path.join(self.image_dir, img_id)
            if not os.path.exists(img_path):
                print(f"Warning: Image not found at {img_path}")
                continue

            image = load_img(img_path)
            image = image.resize(self.image_size, Image.Resampling.LANCZOS)
            image_array = img_to_array(image) / 255.0
            raw_images.append(image_array)  # Store for visualization
            image = tf.keras.applications.resnet50.preprocess_input(image_array * 255.0)

            images.append(image)
            filenames.append(img_id)

        if not images:
            print(f"Warning: No valid images in batch {idx}. Returning empty batch.")
            return np.array([], dtype=np.float32), np.array([], dtype=np.float32), []

        return np.array(images, dtype=np.float32), np.array(raw_images, dtype=np.float32), filenames

"""## Loading Test Dataset & Model Weights"""

# Test data path
test_data_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30"

# Initialize test generator
test_generator = TestImageGenerator(
    image_dir=test_data_path,
    image_size=(224, 224),
    batch_size=1
)

weights_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model/car_updated.weights.h5"
if os.path.exists(weights_path):
    model.load_weights(weights_path)
    print(f"Loaded model weights from {weights_path}")
else:
    raise FileNotFoundError(f"Model weights not found at {weights_path}")

def evaluate_all_test_images(model, generator, threshold=0.85, class_names=['background', 'cake', 'car', 'dog', 'person']):
    for batch_idx in range(len(generator)):
        images, raw_images, filenames = generator[batch_idx]

        if images.size == 0:
            print(f"Skipping empty batch {batch_idx}")
            continue

        probs = model.predict(images, verbose=0)  # Shape: (batch_size, 224, 224, num_classes)
        max_probs = np.max(probs, axis=-1)        # Shape: (batch_size, 224, 224)
        preds = np.argmax(probs, axis=-1)         # Shape: (batch_size, 224, 224)

        # Apply threshold: if max prob < threshold, assign class 0 (background)
        preds[max_probs < threshold] = 0

        for i in range(len(images)):
            pred = preds[i]
            raw_image = raw_images[i]
            filename = filenames[i]

            # Visualization
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(raw_image)
            plt.title(f"Test Image: {filename}")
            plt.axis("off")

            plt.subplot(1, 2, 2)
            plt.imshow(mask_to_rgb(pred))
            plt.title(f"Prediction (Labels: {np.unique(pred)})")
            plt.axis("off")

            plt.tight_layout()
            #plt.savefig(f"test_image_{batch_idx}_{i}_{filename}_segmentation.png")
            plt.show()

            print(f"Predicted labels for {filename}: {np.unique(pred)}")

# Run evaluation on all test images with thresholdingß
evaluate_all_test_images(model, test_generator, threshold=0.85)

def evaluate_all_test_images(model, generator, threshold=0.7, class_names=['background', 'cake', 'car', 'dog', 'person']):
    for batch_idx in range(len(generator)):
        images, raw_images, filenames = generator[batch_idx]

        if images.size == 0:
            print(f"Skipping empty batch {batch_idx}")
            continue

        probs = model.predict(images, verbose=0)  # Shape: (batch_size, 224, 224, num_classes)
        max_probs = np.max(probs, axis=-1)        # Shape: (batch_size, 224, 224)
        preds = np.argmax(probs, axis=-1)         # Shape: (batch_size, 224, 224)

        # Apply threshold: if max prob < threshold, assign class 0 (background)
        preds[max_probs < threshold] = 0

        for i in range(len(images)):
            pred = preds[i]
            raw_image = raw_images[i]
            filename = filenames[i]

            # Visualization
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(raw_image)
            plt.title(f"Test Image: {filename}")
            plt.axis("off")

            plt.subplot(1, 2, 2)
            plt.imshow(mask_to_rgb(pred))
            plt.title(f"Prediction (Labels: {np.unique(pred)})")
            plt.axis("off")

            plt.tight_layout()
            #plt.savefig(f"test_image_{batch_idx}_{i}_{filename}_segmentation.png")
            plt.show()

            print(f"Predicted labels for {filename}: {np.unique(pred)}")

# Run evaluation on all test images with thresholdingß
evaluate_all_test_images(model, test_generator, threshold=0.7)

